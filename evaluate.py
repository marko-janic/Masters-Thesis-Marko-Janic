import math
import os
import datetime

import napari
import torch

import numpy as np
from skimage.feature import peak_local_max
from scipy.optimize import linear_sum_assignment
from tqdm import tqdm

# Local imports
from postprocess import _get_max_preds
from train import find_optimal_assignment_heatmaps, get_targets
from utils import create_folder_if_missing, transform_coords_to_pixel_coords
from plotting import compare_predictions_with_ground_truth, compare_heatmaps_with_ground_truth, compare_heatmaps
from vit_model import get_encoded_image


def read_arguments_file(filepath):
    args_dict = {}
    with open(filepath, 'r') as f_file:
        for line in f_file:
            if ':' in line:
                key_val, value = line.strip().split(':', 1)
                args_dict[key_val.strip()] = value.strip()
    return args_dict


def generate_heatmaps_volume(dataset, vit_model, vit_image_processor, device, latent_dim, model,
                             vit_input_size: int, use_fbp: bool, z_eval_min: int, z_eval_max: int):
    """
    Given the model and dataset this function will go through the volume in the dataset slice by slice and get heatmaps
    generated by the model to construct a 3d volume with heatmaps
    :param dataset:
    :param vit_model:
    :param vit_image_processor:
    :param device:
    :param latent_dim:
    :param model:
    :param vit_input_size:
    :param use_fbp:
    :param z_eval_min
    :param z_eval_max
    :return:
    """

    if use_fbp:
        volume = dataset.grandmodel_fbp
    else:
        volume = dataset.grandmodel
    z_max, y_max, x_max = volume.shape

    if z_eval_max > z_max:
        raise Exception(f"z_eval_max ({z_eval_max}) can't be larger than the volume z ({z_max})")

    output_heatmap_volume = torch.zeros((z_max, 256, 256))  # TODO: replace this 256 with an argument

    for z in tqdm(range(z_eval_min, z_eval_max), desc="Generating heatmaps volume"):
        z_slice = volume[z]

        #plt.imshow(z_slice)
        #plt.title(f"Z slice: {z}, min: {z_slice.min()}, max: {z_slice.max()}")
        #plt.savefig(f"z_slice_{z}")
        #plt.close()

        for y_index in range(math.ceil(y_max / vit_input_size)):
            for x_index in range(math.ceil(x_max / vit_input_size)):
                y_end = min((y_index + 1) * vit_input_size, y_max)
                x_end = min((x_index + 1) * vit_input_size, x_max)
                if y_end - (y_index * vit_input_size) < vit_input_size:
                    y_start = y_end - vit_input_size
                else:
                    y_start = vit_input_size * y_index
                if x_end - (x_index * vit_input_size) < vit_input_size:
                    x_start = x_end - vit_input_size
                else:
                    x_start = vit_input_size * x_index

                sub_micrograph = z_slice[y_start:y_end, x_start:x_end]
                if sub_micrograph.max() > sub_micrograph.min():  # We don't need to normalize if everything is 0
                    sub_micrograph = (sub_micrograph - sub_micrograph.min()) / (sub_micrograph.max() -
                                                                                sub_micrograph.min())
                sub_micrograph = sub_micrograph.repeat(3, 1, 1)

                #plt.imshow(sub_micrograph.permute(1, 2, 0).cpu().numpy())
                #plt.title(f"Sub micrograph y: {y_start}, {y_end}, x: {x_start}, {x_end}, min: {sub_micrograph.min()},"
                #          f"max: {sub_micrograph.max()}")
                #plt.savefig(f"sub_micrograph_{y_start}_{y_end}_{x_start}_{x_end}.png")
                #plt.close()

                encoded_image = get_encoded_image(sub_micrograph, vit_model, vit_image_processor)

                # the 1: is because we don't need the class token
                latent_micrographs = encoded_image['last_hidden_state'].to(device)[:, 1:, :]
                latent_micrographs = latent_micrographs.permute(0, 2, 1).reshape(1, latent_dim, 14, 14)
                outputs = model(latent_micrographs)
                heatmap = outputs["heatmaps"][0, 0]

                # TODO: check if this 0.5 wonky stuff is ok here
                y_start = math.floor(y_start*0.5)
                x_start = math.floor(x_start*0.5)
                y_end = math.floor(y_end*0.5)
                x_end = math.floor(x_end*0.5)
                output_heatmap_volume[z, y_start:y_end, x_start:x_end] = torch.max(heatmap, output_heatmap_volume[z,
                                                                                            y_start:y_end,
                                                                                            x_start:x_end])

    return output_heatmap_volume


def evaluate(args, model, vit_model, vit_image_processor, dataset, test_dataloader, criterion, example_predictions):
    """
    :param args: Needs:
        result_dir
        dataset
        num_particles
        device
        latent_dim
        vit_input_size
        particle_height
        particle_width
        prediction_threshold
    :param model:
    :param vit_model:
    :param vit_image_processor:
    :param dataset:
    :param test_dataloader:
    :param criterion:
    :param example_predictions:
    :return:
    """
    if args.existing_evaluation_folder == "" or args.existing_evaluation_folder is None:
        result_dir = os.path.join(args.result_dir,
                                  f'evaluation_{datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")}')
        create_folder_if_missing(result_dir)
        with open(os.path.join(result_dir, 'arguments.txt'), 'a') as f:
            for arg in vars(args):
                f.write(f"{arg}: {getattr(args, arg)}\n")
    else:
        result_dir = os.path.join("experiments", args.existing_result_folder, args.existing_evaluation_folder)

    experiment_result_dir = args.result_dir

    # --- Begin argument consistency check ---
    allowed_missing_or_different_keys = [
        # Add keys here that are allowed to be missing or different in args when comparing eval to train args
        # Example:
        # "some_optional_key",
        "loss_log_path",
        "prediction_threshold",
        "checkpoint_interval",
        "learning_rate",
        "epochs",
        "device",
        "existing_result_folder",
        "mode",
        "config",
        "result_dir_appended_name",
        "shrec_sampling_points",
        "result_dir",
        "batch_size",
        "prediction_threshold",
        "neighborhood_size",
        "volume_evaluation",
        "existing_evaluation_folder",
        "split_file_name",
        "missing_pred_threshold"
    ]
    arguments_file_evaluation = os.path.join(result_dir, 'arguments.txt')
    arguments_file_experiment = os.path.join(experiment_result_dir, 'arguments.txt')
    if not os.path.exists(arguments_file_experiment):
        raise FileNotFoundError(f"arguments.txt not found in experiment_result_dir: {experiment_result_dir}")

    args_evaluation = read_arguments_file(arguments_file_evaluation)
    args_experiment = read_arguments_file(arguments_file_experiment)

    with open(arguments_file_evaluation, 'a') as log_f:
        for key in args_evaluation:
            if key in allowed_missing_or_different_keys:
                continue  # skip allowed keys for both missing and difference
            if key in args_experiment:
                if args_evaluation[key] != args_experiment[key]:
                    msg = (f"Argument '{key}' differs:\n"
                           f"  evaluation_result_dir value: {args_evaluation[key]}\n"
                           f"  experiment_result_dir value: {args_experiment[key]}")
                    print(msg)
                    if args.existing_evaluation_folder == "" or args.existing_evaluation_folder is None:
                        log_f.write(f"[DIFFERENCE] {msg}\n")
            else:
                msg = f"Argument '{key}' is only present in evaluation arguments.txt"
                print(msg)
                if args.existing_evaluation_folder == "" or args.existing_evaluation_folder is None:
                    log_f.write(f"[MISSING] {msg}\n")
        # Check for keys in experiment args that are missing in evaluation args, unless allowed
        for key in args_experiment:
            if key in allowed_missing_or_different_keys:
                continue  # skip allowed keys for both missing and difference
            if key not in args_evaluation:
                msg = f"Argument '{key}' is present in experiment arguments.txt but missing in evaluation"
                print(msg)
                if args.existing_evaluation_folder == "" or args.existing_evaluation_folder is None:
                    log_f.write(f"[MISSING_IN_EVAL] {msg}\n")
    # --- End argument consistency check ---

    model.eval()
    criterion.eval()
    running_loss = 0.0
    running_pixel_loss = 0  # average amount of pixels that the predictions are off
    running_missing_predictions = 0
    running_extra_predictions = 0
    example_counter = 0

    with (torch.no_grad()):
        # TODO: hopefully we can remove the first part of this if (an abomination) and only keep volume evaluation
        if not args.volume_evaluation:
            for micrographs, index in tqdm(test_dataloader, desc="Evaluating"):
                model.eval()
                criterion.eval()

                target_heatmaps, targets = get_targets(args=args, dataset=dataset, index=index)

                encoded_image = get_encoded_image(micrographs, vit_model, vit_image_processor)
                # the 1: is because we don't need the class token
                latent_micrographs = encoded_image['last_hidden_state'].to(args.device)[:, 1:, :]
                latent_micrographs = latent_micrographs.permute(0, 2, 1).reshape(1, args.latent_dim, 14, 14)

                outputs = model(latent_micrographs)

                missed_predictions = 0
                extra_predictions = 0
                avg_pixels_off = 0

                # We use this to compute the average pixels off so to speak, we set it to -1 to indicate no prediction
                padded_target_boxes = torch.full((1, args.num_particles, 2), -1.0, dtype=torch.float32)
                for i in range(len(targets)):
                    indices = len(targets[i]["boxes"])
                    if indices > args.num_particles:
                        missed_predictions += + indices - args.num_particles
                        indices = args.num_particles
                    padded_target_boxes[i, :indices, :] = targets[i]["boxes"][:indices, :2]
                assignments = find_optimal_assignment_heatmaps(outputs["heatmaps"], target_heatmaps, criterion)
                # We put -1 to signify there is no prediction here
                reordered_padded_target_boxes = torch.full_like(padded_target_boxes, -1.0, dtype=torch.float32)
                reordered_target_heatmaps = torch.zeros_like(target_heatmaps)
                for batch_idx, (row_ind, col_ind) in enumerate(assignments):
                    reordered_target_heatmaps[batch_idx] = target_heatmaps[batch_idx, col_ind]
                    reordered_padded_target_boxes[batch_idx] = padded_target_boxes[batch_idx, col_ind]
                reordered_padded_target_boxes = reordered_padded_target_boxes * args.vit_input_size  # back to pixel coords
                losses = criterion(outputs["heatmaps"], reordered_target_heatmaps)
                running_loss += losses.item()

                heatmap_for_maxima = target_heatmaps[0, 0, :, :].cpu().numpy()
                predictions = torch.from_numpy(peak_local_max(heatmap_for_maxima, min_distance=5,
                                                              threshold_abs=args.prediction_threshold))
                predictions *= 2  # The heatmaps are size 112 x 112 and the actual coordinates are 224 x 224
                predictions[:, 0] = args.vit_input_size - predictions[:, 0]
                predictions = predictions[:, [1, 0]]
                target_coordinates = targets[0]['boxes'][:, :2]*args.vit_input_size

                num_predictions = predictions.shape[0]
                num_targets = target_coordinates.shape[0]

                # Compute pairwise distances (num_predictions, num_targets), p=2 for euclidian distance
                dists = torch.cdist(predictions.float(), target_coordinates.float(), p=2).cpu().numpy()
                # Hungarian algorithm for optimal assignment
                row_ind, col_ind = linear_sum_assignment(dists)
                # Only consider matches where both indices are valid
                matched_dists = dists[row_ind, col_ind]
                avg_pixels_off = matched_dists.mean() if len(matched_dists) > 0 else 0.0

                # Extra predictions: predictions not matched (if more predictions than targets)
                extra_predictions += max(0, num_predictions - num_targets)
                # Missing predictions: targets not matched (if more targets than predictions)
                missed_predictions += max(0, num_targets - num_predictions)

                pred_coords = predictions
                particle_width_height_columns = torch.full((pred_coords.shape[0], pred_coords.shape[1]),
                                                           args.particle_width, device=pred_coords.device)
                pred_coords = torch.cat([pred_coords, particle_width_height_columns], dim=1).unsqueeze(0)

                running_missing_predictions += missed_predictions
                running_extra_predictions += extra_predictions
                running_pixel_loss += avg_pixels_off

                if example_counter < example_predictions:
                    ground_truth = transform_coords_to_pixel_coords(args.vit_input_size, args.vit_input_size,
                                                                    targets[0]['boxes'][:, :4].unsqueeze(0))

                    compare_heatmaps_with_ground_truth(micrograph=micrographs[0].cpu(),
                                                       particle_locations=ground_truth[0],
                                                       heatmaps=outputs["heatmaps"][0],
                                                       heatmaps_title="Model output",
                                                       result_folder_name=f"model_to_ground_truth_heatmaps_comparison_"
                                                                          f"{example_counter}",
                                                       result_dir=result_dir)

                    compare_heatmaps(heatmaps_gt=reordered_target_heatmaps[0],
                                     heatmaps_pred=outputs["heatmaps"][0],
                                     result_folder_name=f"model_heatmaps_vs_target_heatmaps_{example_counter}",
                                     result_dir=result_dir)

                    compare_predictions_with_ground_truth(
                        image_tensor=micrographs[0].cpu(),
                        ground_truth=ground_truth[0],
                        predictions=pred_coords[0],
                        object_type="output_box",
                        object_parameters={"box_width": args.particle_width, "box_height": args.particle_height},
                        result_dir=result_dir,
                        file_name=f'example_{example_counter}.png',
                        figure_title=f"Avg pixels off: {avg_pixels_off}, missing: {missed_predictions}, "
                                     f"extra: {extra_predictions}"
                    )
                    example_counter += 1

            avg_loss = running_loss / len(test_dataloader)
            avg_pixel_loss = running_pixel_loss / len(test_dataloader)
            avg_missed_predictions = running_missing_predictions / len(test_dataloader)
            avg_extra_predictions = running_extra_predictions / len(test_dataloader)

            print(f"Average evaluation loss: {avg_loss}")
            print(f"Average pixel loss: {avg_pixel_loss}")
            print(f"Average number of missed predictions: {avg_missed_predictions}")
            print(f"Average number of extra predictions: {avg_extra_predictions}")
            print(f"prediction_threshold: {args.prediction_threshold}")
            
        else:
            if args.existing_evaluation_folder is None or args.existing_evaluation_folder == "":
                output_heatmaps_volume = generate_heatmaps_volume(dataset=dataset, vit_model=vit_model,
                                                                  vit_image_processor=vit_image_processor,
                                                                  device=args.device, latent_dim=args.latent_dim,
                                                                  model=model, vit_input_size=args.vit_input_size,
                                                                  use_fbp=args.use_fbp, z_eval_max=args.shrec_max_z,
                                                                  z_eval_min=args.shrec_min_z)
                output_heatmaps_volume_numpy = output_heatmaps_volume.cpu().numpy()
                np.save(os.path.join(result_dir, f"output_heatmaps_volume.npy"), output_heatmaps_volume_numpy)
            else:
                output_heatmaps_volume_numpy = np.load(os.path.join(result_dir, "output_heatmaps_volume.npy"))

            coordinates = torch.from_numpy(peak_local_max(output_heatmaps_volume_numpy,
                                                          min_distance=args.neighborhood_size,
                                                          threshold_abs=args.prediction_threshold))
            coordinates[:, 1:] = coordinates[:, 1:] * 2  # Scale them since heatmaps are smaller

            #target_heatmap_volume = dataset.heatmaps_volume
            #viewer = napari.Viewer()
            #viewer.add_points(coordinates, size=5, face_color='red')
            #viewer.add_image(target_heatmap_volume.cpu().numpy(), name='Target Heatmaps Volume', colormap='blue')
            #viewer.add_image(output_heatmaps_volume_numpy, name='Output Heatmaps Volume', colormap='magenta')
            #viewer.add_image(dataset.grandmodel.cpu().numpy(), name='Grandmodel Volume', colormap='gray')
            #napari.run()

            # We take order z, y, x because that's how peak_local_max returns them as well
            if args.shrec_specific_particle is None or args.shrec_specific_particle == "":
                target_coordinates = torch.tensor(dataset.particle_locations[['Z', 'Y', 'X']].values)
            else:
                filtered_particle_locations = dataset.particle_locations[
                    dataset.particle_locations['class'] == args.shrec_specific_particle]
                target_coordinates = torch.tensor(filtered_particle_locations[['Z', 'Y', 'X']].values)

            pred_coords = coordinates.float()
            tgt_coords = target_coordinates.float()
            num_preds = pred_coords.shape[0]
            num_targets = tgt_coords.shape[0]

            # Compute pairwise distances (num_preds, num_targets)
            dists = torch.cdist(pred_coords, tgt_coords, p=2).cpu().numpy()
            # Assign each prediction to the closest target
            pred_to_target = dists.argmin(axis=1)
            pred_to_target_dist = dists[np.arange(num_preds), pred_to_target]

            # Remove predictions that are too far from any target (count as extra)
            assigned_preds = {}
            extra_predictions = 0
            for pred_idx, (tgt_idx, dist) in enumerate(zip(pred_to_target, pred_to_target_dist)):
                if dist > args.missing_pred_threshold:
                    extra_predictions += 1
                else:
                    assigned_preds.setdefault(tgt_idx, []).append((pred_idx, dist))

            missed_predictions = 0
            matched_dists = []
            for tgt_idx in range(num_targets):
                preds_for_target = assigned_preds.get(tgt_idx, [])
                if len(preds_for_target) == 0:
                    missed_predictions += 1
                elif len(preds_for_target) == 1:
                    # One prediction for this target
                    matched_dists.append(preds_for_target[0][1])
                else:
                    # Multiple predictions: pick the closest, others are extra
                    preds_for_target.sort(key=lambda x: x[1])
                    matched_dists.append(preds_for_target[0][1])
                    extra_predictions += len(preds_for_target) - 1

            avg_pixels_off = np.mean(matched_dists) if matched_dists else 0.0

            avg_loss = 0  # TODO: compute mse between target and produced heatmap here
            avg_pixel_loss = avg_pixels_off
            avg_missed_predictions = missed_predictions
            avg_extra_predictions = extra_predictions
            correct_predictions = len(matched_dists)

            print(f"Total number of predictions: {num_preds}")
            print(f"Total number of targets: {num_targets}")
            print(f"Average evaluation loss: {avg_loss}")
            print(f"Average pixel loss: {avg_pixel_loss}")
            print(f"Missed predictions: {avg_missed_predictions}")
            print(f"Extra predictions: {avg_extra_predictions}")
            print(f"Correct predictions: {correct_predictions}")
            print(f"prediction_threshold: {args.prediction_threshold}")
            print(f"neighborhood_size: {args.neighborhood_size}")
            print(f"missing_pred_threshold: {args.missing_pred_threshold}")

            # Logging the running loss to a txt file
            log_file_path = os.path.join(result_dir, "evaluation_log.txt")
            with open(log_file_path, "a") as log_file:
                log_file.write(f"\nEvaluation: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                log_file.write(f"Total number of predictions: {num_preds}")
                log_file.write(f"Average evaluation loss: {avg_loss}\n")
                log_file.write(f"Average pixel loss: {avg_pixel_loss}\n")
                log_file.write(f"Average number of missed predictions: {avg_missed_predictions}\n")
                log_file.write(f"Average number of extra predictions: {avg_extra_predictions}\n")
                log_file.write(f"prediction_threshold: {args.prediction_threshold}\n")
                log_file.write(f"neighborhood_size: {args.neighborhood_size}\n")
                log_file.write(f"missing_pred_threshold: {args.missing_pred_threshold}\n")
